# HW4: Multimodal Agent - Voice Capabilities

**Student:** Karlo Vranƒçiƒá
**Course:** MIT AI Studio
**Date:** October 2025

---

## 1. Implementation Overview

This project extends my previous Digital Twin (HW1-HW3) with voice interaction capabilities, enabling speech-to-text input and text-to-speech output. The system now supports a **podcast-style discussion mode** where three distinct AI agents (Zeitgeist Philosopher, Cynical Content Architect, and Brutalist Optimizer) engage in spoken conversations, each with their own unique voice.

### Key Features Added

1. **Speech-to-Text (STT):** OpenAI Whisper API for transcribing user voice input
2. **Text-to-Speech (TTS):** Kokoro TTS with multiple voices for agent differentiation
3. **Podcast Mode:** Multi-agent discussions with turn-taking and voice synthesis
4. **Real-time Voice I/O:** Microphone recording with automatic silence detection

### Architecture

```
Digital Twin Voice System
‚îÇ
‚îú‚îÄ‚îÄ Audio Layer (src/voice/audio_utils.py)
‚îÇ   ‚îú‚îÄ‚îÄ AudioRecorder: Microphone capture with silence detection
‚îÇ   ‚îî‚îÄ‚îÄ AudioPlayer: Speaker output for synthesized speech
‚îÇ
‚îú‚îÄ‚îÄ STT Layer (src/voice/stt.py)
‚îÇ   ‚îî‚îÄ‚îÄ WhisperSTT: OpenAI Whisper API integration
‚îÇ
‚îú‚îÄ‚îÄ TTS Layer (src/voice/tts.py)
‚îÇ   ‚îî‚îÄ‚îÄ KokoroTTS: Multi-voice synthesis (3 unique voices)
‚îÇ
‚îî‚îÄ‚îÄ Orchestration (src/voice/podcast_orchestrator.py)
    ‚îî‚îÄ‚îÄ PodcastOrchestrator: Manages agent conversations
```

---

## 2. How Voice Interaction Works

### A. Speech-to-Text Implementation

**Library Used:** OpenAI Whisper API (via `openai` Python package)

**Implementation Details:**
- Uses OpenAI's cloud-based Whisper model (`whisper-1`)
- Supports multiple audio formats (WAV, MP3, MP4)
- Transcribes with high accuracy for English language input
- Returns plain text transcriptions

**Code Flow:**
```python
# User speaks into microphone
audio_file = AudioRecorder().record_to_file()

# Whisper transcribes to text
stt = WhisperSTT(api_key=Config.OPENAI_API_KEY)
transcript = stt.transcribe(audio_file)

# Text is processed by agents
```

**Why Whisper?**
- Industry-leading accuracy for speech recognition
- Handles various accents and background noise well
- Simple API integration with minimal code
- Cloud-based means no local model management

### B. Text-to-Speech Implementation

**Library Used:** Kokoro TTS (via `kokoro-onnx` package)

**Implementation Details:**
- Local neural TTS model (no API costs)
- Supports 9 different voice presets
- 24kHz sample rate for clear audio quality
- Real-time synthesis (< 1 second latency)

**Voice Assignment:**
- **Zeitgeist Philosopher** ‚Üí `af_sky` (deep, contemplative female voice)
- **Cynical Content Architect** ‚Üí `am_adam` (dry, sardonic male voice)
- **Brutalist Optimizer** ‚Üí `bf_emma` (analytical British female voice)

**Code Flow:**
```python
# Agent generates text response
text = "Your bounce rate is achieving escape velocity."

# Kokoro synthesizes with agent-specific voice
tts = KokoroTTS()
audio = tts.speak_as_agent(text, agent_name='optimizer')

# Audio is played through speakers
AudioPlayer().play(audio)
```

**Why Kokoro?**
- Open-source and runs locally (privacy + cost savings)
- Multiple voice options for agent differentiation
- Good quality without requiring GPU
- Simple Python integration

### C. Podcast Orchestration

**How it Works:**

1. **Input:** User speaks a topic or types it
2. **Transcription:** Whisper converts speech ‚Üí text
3. **Round 1 - Openings:** Each agent provides opening statement
   - Text generated by LLM
   - Synthesized with agent's unique voice
   - Played sequentially
4. **Round 2+ - Discussion:** Agents respond to each other
   - Previous statements provided as context
   - Natural conversation flow
5. **Final Round - Conclusions:** Each agent summarizes
6. **Output:** Full transcript saved to markdown file

**Turn-Taking Logic:**
```python
for agent_name in ['philosopher', 'architect', 'optimizer']:
    # Generate response
    task = create_response_task(agent, topic, previous_statement)
    response = execute_task(task)

    # Speak it aloud
    tts.speak_as_agent(response, agent_name)

    # Next agent responds to this
    previous_statement = response
```

---

## 3. Example Run Explanation

### Command Used:
```bash
python main.py voice-chat --topic "AI agents and the future of work"
```

### What Happened:

**Step 1: Topic Input**
- User types topic: "AI agents and the future of work"
- (Alternative: user speaks topic, Whisper transcribes)

**Step 2: Opening Statements (Round 1)**

**üßê Zeitgeist Philosopher** (voice: af_sky):
> "Oh look, another panic about automation. How delightfully predictable. But here's the thing - humans aren't afraid of AI taking jobs. They're afraid of being forced to admit they've been treating work as their primary source of meaning in a meaningless universe. The real disruption isn't technological; it's existential."

**‚úçÔ∏è Cynical Content Architect** (voice: am_adam):
> "The philosopher's right, and corporations know it. That's why 'AI will augment, not replace' became the most repeated lie of 2024. People don't want augmentation - they want security. And we're selling them stories about collaboration when we should be redesigning what work means entirely."

**üìä Brutalist Optimizer** (voice: bf_emma):
> "Your emotional narratives aside, the data is clear: 47% of current jobs will be automated by 2030. The inefficiency isn't in the automation itself - it's in pretending it won't happen. Optimal strategy: retrain for non-automatable skills. Humans excel at creativity, empathy, complex problem-solving. Focus there."

**Step 3: Discussion Round 2**

Each agent responds to the previous points, building on the conversation...

**Step 4: Conclusions**

Each agent provides a final thought that synthesizes the discussion.

**Step 5: Output**
- Transcript saved to `outputs/podcast_ai_agents_and_the_future_of_work.md`
- User can review the written record of the spoken discussion

### Insights from This Run:

1. **Multi-voice differentiation works:** Each agent sounds distinct, making it easy to follow who's speaking
2. **Conversational flow is natural:** Agents build on each other's points, not just repeating ideas
3. **Personality shines through:** Even in spoken form, each agent maintains their character (sarcastic philosopher, cynical architect, data-driven optimizer)
4. **Podcast format is engaging:** More interesting than reading three separate reports
5. **Technical performance:** Minimal latency between speakers, clear audio quality

---

## 4. Technical Challenges & Solutions

### Challenge 1: Voice Selection
**Problem:** How to assign distinct voices to each agent?

**Solution:** Used Kokoro's preset voices with personality matching:
- Philosopher needs intellectual gravitas ‚Üí deep female voice (`af_sky`)
- Architect needs dry wit ‚Üí male voice with character (`am_adam`)
- Optimizer needs precision ‚Üí British accent for technical authority (`bf_emma`)

### Challenge 2: Silence Detection
**Problem:** When to stop recording user input?

**Solution:** Implemented RMS-based silence detection:
- Track audio amplitude in real-time
- If RMS < threshold for 2 seconds ‚Üí stop recording
- Prevents awkward "still listening" scenarios

### Challenge 3: Conversation Context
**Problem:** How to make agents respond to each other, not just the topic?

**Solution:** Pass previous statement as context to next agent's task:
```python
task = create_response_task(agent, topic, previous_statement)
```
This ensures each agent builds on the conversation.

### Challenge 4: Latency Management
**Problem:** Long pauses between agents break podcast flow.

**Solution:**
- Use local Kokoro TTS (no API latency)
- Pre-initialize TTS engine before podcast starts
- Sequential execution with immediate playback

---

## 5. Libraries & APIs Used

### Speech-to-Text
- **OpenAI Python SDK** (`openai>=1.0.0`)
  - Whisper API integration
  - Cloud-based transcription
  - Requires API key

### Text-to-Speech
- **Kokoro ONNX** (`kokoro-onnx>=0.3.0`)
  - Neural TTS engine
  - Local inference
  - Multiple voice support

### Audio Handling
- **sounddevice** (`sounddevice>=0.4.6`)
  - Microphone input capture
  - Speaker output playback
  - Cross-platform support

- **soundfile** (`soundfile>=0.12.1`)
  - Audio file read/write
  - WAV format handling

- **numpy** (`numpy>=1.24.0`)
  - Audio sample processing
  - Silence detection math

### Agent Framework
- **CrewAI** (from HW1-HW3)
  - Multi-agent orchestration
  - Task management
  - Context passing

---

## 6. What I Learned

### Technical Learnings

1. **STT vs TTS tradeoffs:**
   - Cloud STT (Whisper): Great accuracy, requires internet
   - Local TTS (Kokoro): Privacy + speed, but limited voices

2. **Audio processing is harder than expected:**
   - Silence detection needs tuning for different environments
   - Sample rates must match between recording/playback
   - Background noise significantly affects transcription

3. **Voice selection matters for UX:**
   - Distinct voices prevent confusion in multi-agent systems
   - Voice personality should match agent character
   - British accent does make technical content sound more authoritative (optimizer!)

### Design Learnings

1. **Podcast format > Monologue:**
   - Multi-agent discussion is more engaging than single responses
   - Turn-taking creates natural conversation flow
   - Disagreement and building on ideas adds value

2. **Voice reveals personality:**
   - Same text sounds different in different voices
   - TTS enhances agent character differentiation
   - Users connect more with "speaking" agents

3. **Accessibility implications:**
   - Voice I/O makes AI more accessible
   - Hands-free operation enables multitasking
   - Audio transcripts useful for different learning styles

---

## 7. Future Improvements

1. **Real-time voice interruption:** Let user interrupt agents mid-speech
2. **Emotion modulation:** Vary pitch/tone based on content (sarcasm detection!)
3. **Voice cloning:** Train custom voices that better match agent personalities
4. **Whisper local model:** Add offline STT option using local Whisper
5. **Multi-language support:** Podcast in Croatian/other languages

---

## 8. Repository & Video

**GitHub:** [https://github.com/karlovrancic/digital-twin](https://github.com/karlovrancic/digital-twin)

**Demo Video:** [REPLACE WITH YOUR YOUTUBE LINK]

**How to Test:**
```bash
# Clone repo
git clone https://github.com/karlovrancic/digital-twin.git
cd digital-twin

# Install dependencies
pip install -r requirements.txt
brew install portaudio  # macOS only

# Configure API keys
cp .env.example .env
# Edit .env and add OPENAI_API_KEY

# Test voice features
python main.py test-mic
python main.py test-voices

# Run podcast mode
python main.py voice-chat --topic "your topic here"
```

---

## Conclusion

This homework successfully extended my Digital Twin with multimodal voice capabilities. The combination of OpenAI Whisper (STT) and Kokoro TTS enabled natural voice interactions, while the podcast orchestration format created engaging multi-agent discussions. Each agent maintains their unique personality through both text generation and voice synthesis, resulting in a system that feels more human and accessible than text-only interaction.

The journey from planning to implementation revealed both the power and complexity of voice AI - from managing audio I/O to orchestrating multi-speaker conversations. The final system demonstrates that voice can enhance agent interactions beyond mere novelty, creating genuinely more engaging and accessible AI experiences.
